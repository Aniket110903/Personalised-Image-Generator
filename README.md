#  Personalised Image Generation 
 
## **Overview**
Flux LoRA Optimization is a project aimed at fine-tuning the **Flux generative AI model** using **Low-Rank Adaptation (LoRA)**. The primary goal is to create a scalable, efficient, and resource-friendly pipeline for adapting large pre-trained models to generate high-quality artistic visuals. This project leverages advanced tools such as **Replicate**, **Hugging Face**, **NVIDIA A100 GPUs**, and **PyTorch**.

---

## **Features**
- Efficient fine-tuning using **LoRA**, which reduces memory and computational requirements.
- Integration with **Hugging Face datasets** for diverse and high-quality training data.
- Scalable distributed training with **Replicate** on **NVIDIA A100 GPUs**.
- Modular pipeline for dataset preparation, model training, and evaluation.
- Applications in digital art, animation, marketing, and virtual reality.

---

## **Technologies Used**
- **Programming Language**: Python
- **Frameworks**: PyTorch, Hugging Face Transformers
- **Hardware**: NVIDIA A100 GPUs
- **Platforms**: Replicate
- **Development Environment**: Visual Studio Code (VS Code)
- **Containerization**: Docker

---

## **Project Workflow**
The project consists of the following stages:

1. **Project Initialization**:
   - Define objectives, identify requirements, and set up the environment.
   - Tools: VS Code, Python virtual environment, Docker.

2. **Dataset Preparation**:
   - Select datasets (e.g., from Hugging Face).
   - Preprocess data (resize, normalize, and augment images).
   - Validate dataset quality and diversity.

3. **Model Selection and Integration**:
   - Choose the pre-trained **Flux** model.
   - Integrate **LoRA** layers for efficient fine-tuning.

4. **Training and Fine-Tuning**:
   - Perform training using distributed GPUs on **Replicate**.
   - Optimize training with **mixed precision** and hyperparameter tuning.

5. **Evaluation and Testing**:
   - Validate model performance on diverse datasets.
   - Test model outputs for quality and alignment with objectives.

6. **Deployment**:
   - Deploy the fine-tuned model for applications in digital art, branding, and prototyping.

---

## **Installation**

### Prerequisites
- Python 3.8+
- NVIDIA GPU with CUDA support (optional but recommended)
- Docker (optional for containerization)

### Steps
1. Clone the repository:
   ```bash
   git clone https://github.com/username/flux-lora-optimization.git
   cd flux-lora-optimization

## **Results**

![1](https://github.com/Aniket110903/Personalised-Image-Generator/blob/main/Results/1.jpeg)
